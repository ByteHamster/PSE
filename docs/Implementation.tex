\documentclass[parskip=full]{scrartcl}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[german]{babel}
\usepackage{hyperref}
\usepackage[toc, nonumberlist]{glossaries}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{float}
\usepackage{color}

\hypersetup{
  colorlinks=false,
  linktoc=all,
  hidelinks,
}

\title{OSIP - Implementierung}
\subtitle{OPC UA Simulator for Industrial Plants}
\author{
    M. Armbruster\\
    D. Kahles\\
    H. Lehmann\\
    M. Schwarzmann\\
    N. Wilhelm
}

\begin{document}
\maketitle
\thispagestyle{empty}
\vspace{20px}
\begin{center}
  \includegraphics[scale=0.4]{../icon.png}
\end{center}
\pagebreak
\tableofcontents
\pagebreak

\section{Einleitung}
Nachdem im Designdokument der softwaretechnische Entwurf von OSIP festgelegt wurde, wird in diesem Dokument erläutert, wie die Implementierung von OSIP auf Basis
dieses Entwurfs verlaufen ist.

Hierbei wird zuerst kurz der interne Workflow erklärt. Anschließend wird aufgezeigt, in welche Abschnitte die Implementierung von OSIP aufgeteilt wurde und
welches Teammitglied sich hauptsächlich um welche Teile der Implementierung bemüht hat.

Im Anschluss wird erläutert, an welchen Stellen der Entwurf auf welche Art geändert werden musste, um in der Praxis auftretenden Problemen zu begegnen. Danach
werden die im Pflichtenheft definierten Muss- und Kannkriterien sowie die im Design daraus getroffene Auswahl noch einmal mit der tatsächlich implementierten
Funktionalität abgeglichen. Abweichungen vom ursprünglichen Zeitplan der Implementierung werden ebenfalls erläutert.

Als Abschluss werden kurz Probleme in der Implementierung aufgelistet und ein Überblick zu den vorhandenen Unittests gegeben.

\section{Workflow}
OSIP wurde von uns open-source mit Hilfe von GitHub entwickelt. Um die Qualität des Codes möglichst gut zu halten,
haben wir konsequent Code-Reviews eingesetzt. Jede Änderung musste von einem Kommilitonen genehmigt werden. Zusätzlich ließen wir
durch den Dienst "`Travis"' bei jeder einzubindenden Änderung die Stil-Richtlinien für den Code (mittels Checkstyle) und die Unit Tests überprüfen.
Dieses Vorgehen erforderte zwar oft lange Wartezeiten auf Reviews, im Sinne der guten Qualität haben wir uns allerdings darauf eingelassen. 

Um die Aufgaben der einzelnen Kommilitonen aufeinander abzustimmen, setzten wir den von GitHub bereitgestellten Issue-Tracker ein. Im Laufe der
Entwicklung von OSIP wurden über 400 Issues und Pull requests erstellt.

\section{Arbeitsaufteilung}
Bei der Implementierung gingen wir nach dem outside-in-Prinzip vor. Während Model, OPC UA Wrapper und Core-Bestandteile von David, Hans-Peter und Max entwickelt wurden,
begannen Martin und Niko mit den Benutzeroberflächen von Überwachungskonsole und Fertigungssimulation. Aufgrund des modularen Entwurfes
und der automatischen Generierung von Quellcode aus dem Klassendiagramm war dies problemlos möglich. Die Aufteilung erfolgte deshalb, weil Martin und Niko die
Benutzeroberflächen während der letzten Phase entworfen hatten und sich somit besonders gut in dem Bereich auskannten. Des Weiteren hätten wir uns mit 5 Personen,
die das Model entwickeln, wahrscheinlich gegenseitig sehr gestört.

Nachdem dann die Bestandteile einzeln implementiert waren, wurden noch die Controller hinzugefügt. Die Controller verbinden jeweils Model und View,
um so Leben in die Anwendung zu bringen. Da zu diesem Zeitpunkt das Model mit der Grundfunktionalität der Anwendungen schon fertig war,
konnte man in den Controllern sehr schnell Fortschritte sehen. Die Controller wurden maßgeblich von David und Max entwickelt.

Insgesamt ließ sich über diese Aufteilung eine weitestgehend gerechte Arbeitsaufteilung umsetzen.

\section{Änderungen zum Entwurf}


\section{Implementierte Muss- und Kannkriterien}
In diesem Abschnitt werden die im Pflichtenheft definierten Kann- und Musskriterien noch einmal kurz umrissen und erläutert, welche davon erfolgreich
implementiert wurden oder nicht und wodurch sich das jeweils begründet.

Die Musskriterien konnten wir nahezu vollständig in der Form in der sie im Pflichtenheft stehen im fertigen Programm realisieren.

Die einzige Abweichung besteht in der Übertragung der Alarmzustände an die Überwachungskonsole. In den Funktionalen Anforderungen (FA 440) wurde definiert,
dass diese asynchron zum normalen Aktualisierungsintervall der Überwachungskonsole empfangen werden. Die von uns verwendete OPC UA Implementierung Milo
stellt die dazu nötige Funktionalität leider nicht bereit. Darum haben wir uns entschlossen die Alarme zwar nur in regelmäßigen Intervallen zu übermitteln,
dieses Intervall aber sehr klein gewählt, um die Asynchronität zu emulieren.

Innerhalb der geplanten Kannkriterien kam es ebenfalls zu leichten Abweichungen.

Die Logging-Konsole innerhalb der Überwachungskonsole wurde auf zwei separate Tabs aufgeteilt. Der erste Tab loggt wie geplant die von der Fertigungssimulation
eintreffenden Alarme. Der zweite Tab loggt auftretende Ausnahmen und Fehlermeldungen, anstatt diese wie ursprünglich geplant auf stdout auszugeben. So sind die
Ausnahmen und Fehlermeldungen auch dann für den Anwender sichtbar, wenn er die beiden Programme nicht über ein Terminal startet, auf dem die Ausgabe erfolgen kann.
Zudem dient die Zweiteilung in Tabs der Übersichtlichkeit. Potentiell auftretende Ausnahmen im Programmfluss verdrängen so nicht OPC UA Alarme.

Des weiteren wurde der Jitter in den Eingabewerten nicht implementiert. Praxistests haben ergeben, dass der Jitter entweder so schwach ist, dass er kaum sichtbar
ist und somit nur unnötigen Overhead in der Fertigungssimulation erzeugt, oder aber so stark ist, dass er immer wieder zu nicht geplanten und nicht vorhersehbaren
Überläufen während der Ausführung von Szenarien führt.

\section{Verzögerungen im Implementierungsplan}
Die Implementierungsphase war zunächst auf einen Monat ausgelegt.
In der ersten Woche sollten das Model, die OPC UA Wrapper, Core und einige Bestandteile der Benutzeroberflächen
implementiert werden.
In der zweiten Woche sollten Settingsfenster und weitere Dialogfenster implementiert werden.
In der dritten Woche sollten die Controller für die Simulation und Überwachungskonsole implementiert werden.
Die letzte Woche war zur Beseitigung von verbleibenden Fehlern gedacht.

Um den Implementierungsplan zu entspannen wurde bereits zwei Wochen vor Beginn der eigentlichen Implementierungsphase
mit der Arbeit am Parser für die Szenarien aus dem Core begonnen. Dieser wurde zu Beginn der Implementierungsphase fertiggestellt.
Auch die OPC UA Wrapper wurden eine halbe Woche vor Beginn der Implementierungsphase angefangen, da aufgrund der mangelnden Dokumentation von Milo Verzug im OPC UA Wrapper bereits eingeplant war.

Dennoch gab es an einigen Stellen starke Verzögerungen in der Implementierung.
Die Implementierung der Benutzeroberflächen bremste die Entwicklung häufig aus, da Designentscheidungen oft lange
diskutiert wurden und die Ausarbeitung des gewünschten Designs viel Zeit beanspruchte, bis es so aussah wie geplant.
Insbesondere die Controller der Simulation und Überwachungskonsole hatten eine höhere Komplexität als erwartet.
Da sie alle Funktionalitäten mit der Benutzeroberfläche verknüpfen, mussten viele Besonderheiten der OPC UA Server-Client Kommunikation und Sonderfälle berücksichtigt werden.

\section{Probleme bei der Implementierung}
Im Pflichtenheft war geplant, die von OPC UA bereitgestellten Alarme zu benutzen, um bei einem ungewöhnlichen Zustand (z.B. Überlauf)
die Überwachungskonsole zu benachrichtigen. Leider unterstützt der Milo Server keine Alarme, siehe auch die entsprechende Antwort
des Milo-Entwicklers auf Stack Overflow: \href{http://stackoverflow.com/a/42161092/}{http://stackoverflow.com/a/42161092/}.
Somit mussten wir auch bei den Alarmen auf reguläre Subscriptions zurückgreifen, welche wir nun mit einem kürzeren Intervall
als die restlichen Produktionsvariablen abfragen.

\section{Unittests}
Unittests waren ausschlaggebend wichtig für den ersten Teil der Implementierung, bei dem die Controller noch nicht entwickelt waren.
Entsprechend ist die Testabdeckung sehr hoch. Das Model besitzt eine Zeilenabdeckung von 93\%, ebenso wie die Core-Bestandteile.
Um die Funktionalität von OPC UA Server und Client zu testen, musste von reinen Unittests abgewichen werden, da der Client keine
Methoden zum Setzen von Werten besitzt. Es wurde somit ein weiteres Modul hinzugefügt, das die Tests der Server- und Client-Module übernimmt.
Dieses Modul testet gleichzeitig das Client-Modul und das Server-Modul zu OPC UA, stellt also eher einen Integrationstest dar. Zur bottom-up
Entwicklung der OPC UA Abstraktion war dies aber unumgänglich. Durch die Tests konnte bei OPC UA eine Abdeckung von 78\% erreicht werden.

In UI und Controller wurden keine automatischen Unittest eingesetzt, sondern die Funktionalität in den entsprechenden Pull requests
jeweils manuell getestet. Wir entschieden uns zu diesem Vorgehen, da das Testen in der UI einen hohen Aufwand bedeutet, der sich allerdings
kaum bezahlt macht - Es gibt kaum hinreichend komplexe, testbare Code-Abschnitte, bei denen sich automatisierte Tests des UI lohnen würden.

\pagebreak
\phantomsection
\addcontentsline{toc}{section}{\listfigurename}
\listoffigures

\end{document}
